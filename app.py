import streamlit as st
import os
import glob
from dotenv import load_dotenv

# Import standard LangChain components
from langchain.schema import Document as LangChainDocument
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain

# Import components from the community packages
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_groq import ChatGroq

# Import PyPDF2 for PDF loading
try:
    from PyPDF2 import PdfReader
except ImportError:
    st.error("‚ùå ‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á PyPDF2: pip install PyPDF2")
    st.stop()


# Load environment variables
load_dotenv()
GROQ_API_KEY = os.getenv("GROQ_API_KEY")

# --- Configuration Constants ---
PDF_FOLDER = "./pdf"
EMBEDDING_MODEL = "all-MiniLM-L6-v2"
LLM_MODEL = "llama-3.1-8b-instant"
SEARCH_LIMIT = 2      # Reduce to 2 to help with token size
MAX_DOC_LENGTH = 1500 # Limit context per document

# -----------------------------
# Load PDF as text
# -----------------------------
def load_pdf_texts(folder):
    """Loads text from all PDF files in the specified folder and truncates content."""
    documents = []
    # Check if the folder exists
    if not os.path.exists(folder):
        st.error(f"‚ö†Ô∏è ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÑ‡∏°‡πà‡∏û‡∏ö: {folder}")
        return documents

    for pdf_file in glob.glob(f"{folder}/*.pdf"):
        try:
            reader = PdfReader(pdf_file)
            text = ""
            for page in reader.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
            
            # Truncate text if too long
            text = text[:MAX_DOC_LENGTH]
            if text.strip():
                documents.append(LangChainDocument(page_content=text, metadata={"source": pdf_file}))
        except Exception as e:
            st.warning(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå PDF: {pdf_file} (‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e})")
            continue
            
    return documents

# -----------------------------
# RAG Chatbot Class
# -----------------------------
class RAGChatbot:
    def __init__(self):
        self.memory = ConversationBufferMemory(
            memory_key="chat_history", 
            return_messages=True,
            input_key="question"
        )
        self.embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDING_MODEL,
            model_kwargs={'device': 'cpu'} # ‡πÉ‡∏ä‡πâ CPU ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£
        )
        self.vector_store = None
        self.qa_chain = None
        self.conversation_chain = None
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö API Key ‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
        if not GROQ_API_KEY:
            st.error("‚ùå ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ GROQ_API_KEY ‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå .env")
            st.stop()
            
        self.llm = ChatGroq(groq_api_key=GROQ_API_KEY, model_name=LLM_MODEL, temperature=0.1)

    def load_documents(self, pdf_folder=PDF_FOLDER):
        documents = load_pdf_texts(pdf_folder)
        if not documents:
            # st.error ‡∏ñ‡∏π‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ô load_pdf_texts ‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏≤‡∏Å folder ‡πÑ‡∏°‡πà‡∏û‡∏ö
            if os.path.exists(pdf_folder) and not glob.glob(f"{pdf_folder}/*.pdf"):
                 st.error(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ PDF ‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå: {pdf_folder}")
            return False
            
        self.vector_store = FAISS.from_documents(documents, embedding=self.embeddings)

        # --- 1. RetrievalQA Chain Setup ---
        prompt_template = """
‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏û‡∏∑‡πâ‡∏ô‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô
‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á:
{context}

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {question}
‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:"""
        PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])

        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vector_store.as_retriever(search_kwargs={"k": SEARCH_LIMIT}),
            chain_type_kwargs={"prompt": PROMPT},
            return_source_documents=True
        )

        # --- 2. ConversationalRetrievalChain Setup (‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î) ---
        # ‡∏•‡∏ö chain_type_kwargs={"output_key": "answer"} ‡∏≠‡∏≠‡∏Å
        self.conversation_chain = ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=self.vector_store.as_retriever(search_kwargs={"k": SEARCH_LIMIT}),
            memory=self.memory,
            return_source_documents=True,
        )
        return True

    def answer_question(self, question, use_conversation=True):
        """Answers the user's question using the selected chain."""
        if use_conversation and self.conversation_chain:
            # ‡πÉ‡∏ä‡πâ invoke ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ConversationalRetrievalChain
            result = self.conversation_chain.invoke({"question": question}) 
            answer = result.get("answer", "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏´‡∏≤‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÑ‡∏î‡πâ") # ‡πÉ‡∏ä‡πâ .get() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô KeyError
            sources = result.get("source_documents", [])
        else:
            # ‡πÉ‡∏ä‡πâ invoke ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RetrievalQA Chain
            result = self.qa_chain.invoke({"query": question}) 
            answer = result.get("result", "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏´‡∏≤‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÑ‡∏î‡πâ")
            sources = result.get("source_documents", [])
        return {"answer": answer, "sources": sources}

# -----------------------------
# Streamlit UI
# -----------------------------
def main():
    st.set_page_config(page_title="Chatbot ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏ô‡πà‡∏≤‡∏ô", layout="wide")
    st.title("üçΩÔ∏è Chatbot ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏ô‡πà‡∏≤‡∏ô")
    st.markdown("---")

    if "bot" not in st.session_state:
        # Initialize and load documents only once
        st.session_state.bot_error = False
        with st.spinner("‚è≥ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£..."):
            try:
                bot = RAGChatbot()
                if bot.load_documents():
                    st.session_state.bot = bot
                else:
                    # ‡∏ñ‡πâ‡∏≤‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏£‡πâ‡∏≤‡∏¢‡πÅ‡∏£‡∏á (‡πÄ‡∏ä‡πà‡∏ô‡πÅ‡∏Ñ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå)
                    st.session_state.bot_error = True 
            except Exception as e:
                # ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏£‡πâ‡∏≤‡∏¢‡πÅ‡∏£‡∏á‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô ‡πÄ‡∏ä‡πà‡∏ô API Key ‡∏´‡∏£‡∏∑‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•
                st.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô Chatbot: {e}")
                st.stop()
    
    # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÅ‡∏à‡πâ‡∏á error ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ (‡πÅ‡∏ï‡πà bot ‡∏¢‡∏±‡∏á‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏≠‡∏¢‡∏π‡πà)
    if "bot_error" in st.session_state and st.session_state.bot_error:
        # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡πÇ‡∏Ñ‡πâ‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏≠‡∏∑‡πà‡∏ô‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ vector_store
        if "bot" in st.session_state and not st.session_state.bot.vector_store:
             st.info("‚ÑπÔ∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ PDF ‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÑ‡∏ü‡∏•‡πå PDF ‡πÉ‡∏ô `./pdf` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô.")
        return

    bot = st.session_state.bot
    
    # ------------------ Chat History Display ------------------
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Keep only last 5 messages to reduce token usage
    if len(st.session_state.messages) > 10: # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏õ‡πá‡∏ô 10 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô
        st.session_state.messages = st.session_state.messages[-10:]

    # Display history messages
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])
            if msg.get("sources"):
                with st.expander("üìö ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á"):
                    for s in msg["sources"]:
                        source_path = s.metadata.get("source", "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏")
                        file_name = os.path.basename(source_path) 
                        st.markdown(f"**‡πÑ‡∏ü‡∏•‡πå:** `{file_name}`")
                        st.write(s.page_content[:300] + "...") 
                        st.markdown("---")

    # ------------------ Handle new user input ------------------
    if prompt := st.chat_input("‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏ô‡πà‡∏≤‡∏ô..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("ü§ñ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡∏¥‡∏î‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö..."):
                try:
                    # ‡πÉ‡∏ä‡πâ conversational chain ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å
                    response = bot.answer_question(prompt, use_conversation=True) 
                    st.markdown(response["answer"])
                    
                    sources = response.get("sources", [])
                    if sources:
                        with st.expander("üìö ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á"):
                            for s in sources:
                                source_path = s.metadata.get("source", "‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏")
                                file_name = os.path.basename(source_path)
                                st.markdown(f"**‡πÑ‡∏ü‡∏•‡πå:** `{file_name}`")
                                st.write(s.page_content[:300] + "...")
                                st.markdown("---")
                                
                    st.session_state.messages.append({
                        "role": "assistant",
                        "content": response["answer"],
                        "sources": sources
                    })
                except Exception as e:
                    st.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏Ç‡∏ì‡∏∞‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {e}")
                    st.session_state.messages.pop() # ‡∏•‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏≠‡∏≠‡∏Å‡∏´‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î

if __name__ == "__main__":
    main()
